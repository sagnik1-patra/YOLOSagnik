{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a3cb04-9a1a-47b1-a44a-6d2dc80abd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stop button clicked.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Initialize Mediapipe ---\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# --- Load YOLOv10n model ---\n",
    "yolo = YOLO(\"yolov8n.pt\")  # Change to yolov10n.pt when available\n",
    "\n",
    "# Eye landmarks\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "score_history = deque(maxlen=10)\n",
    "distraction = 0\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_points, image_w, image_h):\n",
    "    p = []\n",
    "    for idx in eye_points:\n",
    "        lm = landmarks[idx]\n",
    "        x, y = int(lm.x * image_w), int(lm.y * image_h)\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "        p.append((x, y))\n",
    "\n",
    "    A = np.linalg.norm(np.array(p[1]) - np.array(p[5]))\n",
    "    B = np.linalg.norm(np.array(p[2]) - np.array(p[4]))\n",
    "    C = np.linalg.norm(np.array(p[0]) - np.array(p[3]))\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def is_blinking(ear, threshold=0.2):\n",
    "    return ear < threshold\n",
    "\n",
    "def compute_concentration_score(gaze, head_pose, blink):\n",
    "    score = 0.4 * gaze + 0.4 * head_pose + 0.2 * (0 if blink else 1)\n",
    "    return round(score * 100, 2)\n",
    "\n",
    "def bar(score, frame):\n",
    "    \"\"\"Draw concentration bar\"\"\"\n",
    "    bar_x, bar_y = 30, 100\n",
    "    bar_width, bar_height = 200, 30\n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)\n",
    "    fill_width = int(score * bar_width / 100)\n",
    "    color = (0, 255, 0) if score > 40 else (0, 100, 255)\n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + fill_width, bar_y + bar_height), color, -1)\n",
    "    cv2.putText(frame, f\"{score}%\", (bar_x + bar_width + 10, bar_y + bar_height // 2 + 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "def draw_stop_button(frame):\n",
    "    \"\"\"Draw a STOP button on the frame\"\"\"\n",
    "    button_color = (0, 0, 255)\n",
    "    button_text = \"STOP\"\n",
    "    button_pos = (frame.shape[1] - 120, 20)\n",
    "    button_size = (100, 50)\n",
    "    cv2.rectangle(frame, button_pos, (button_pos[0] + button_size[0], button_pos[1] + button_size[1]), button_color, -1)\n",
    "    cv2.putText(frame, button_text, (button_pos[0] + 10, button_pos[1] + 35),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    return button_pos, button_size\n",
    "\n",
    "def is_button_clicked(event, x, y, flags, param):\n",
    "    global stop_requested\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        bx, by, bw, bh = param\n",
    "        if bx <= x <= bx + bw and by <= y <= by + bh:\n",
    "            print(\"[INFO] Stop button clicked.\")\n",
    "            stop_requested = True\n",
    "\n",
    "# --- Start camera ---\n",
    "cap = cv2.VideoCapture(0)\n",
    "stop_requested = False\n",
    "\n",
    "cv2.namedWindow(\"Concentration Tracker\")\n",
    "cv2.setMouseCallback(\"Concentration Tracker\", is_button_clicked, param=(0, 0, 0, 0))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or stop_requested:\n",
    "        break\n",
    "\n",
    "    # Draw UI background\n",
    "    ui_bg = frame.copy()\n",
    "    cv2.rectangle(ui_bg, (0, 0), (frame.shape[1], 150), (30, 30, 30), -1)\n",
    "    cv2.addWeighted(ui_bg, 0.6, frame, 0.4, 0, frame)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_h, image_w, _ = frame.shape\n",
    "\n",
    "    # Mediapipe face mesh\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                face_landmarks,\n",
    "                mp_face_mesh.FACEMESH_CONTOURS\n",
    "            )\n",
    "            landmarks = face_landmarks.landmark\n",
    "            left_ear = eye_aspect_ratio(landmarks, LEFT_EYE, image_w, image_h)\n",
    "            right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE, image_w, image_h)\n",
    "            avg_ear = (left_ear + right_ear) / 2\n",
    "            blink = is_blinking(avg_ear)\n",
    "\n",
    "            gaze_score = 1  # Placeholder (you can add actual gaze detection logic)\n",
    "            head_score = 1  # Placeholder (you can add head pose detection logic)\n",
    "            concentration = compute_concentration_score(gaze_score, head_score, blink)\n",
    "\n",
    "            score_history.append(concentration)\n",
    "            smooth_score = int(np.mean(score_history))\n",
    "            bar(smooth_score, frame)\n",
    "\n",
    "            cv2.putText(frame, f\"Concentration: {smooth_score}%\", (30, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            if blink:\n",
    "                cv2.putText(frame, \"BLINKING\", (30, 170),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 150, 255), 2)\n",
    "\n",
    "    # YOLO object detection (phones and people)\n",
    "    yolo_results = yolo.predict(source=frame, save=False, conf=0.3, verbose=False)\n",
    "    for r in yolo_results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            label = yolo.names[cls]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            conf = box.conf[0]\n",
    "            if label in [\"cell phone\", \"person\"]:  # Detect phones and people\n",
    "                color = (0, 0, 255) if label == \"cell phone\" else (255, 255, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Draw stop button\n",
    "    button_pos, button_size = draw_stop_button(frame)\n",
    "    cv2.setMouseCallback(\"Concentration Tracker\", is_button_clicked,\n",
    "                         param=(button_pos[0], button_pos[1], button_size[0], button_size[1]))\n",
    "\n",
    "    cv2.imshow(\"Concentration Tracker\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or stop_requested:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1968e017-a0c2-492a-8c14-e3564c441b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Detection Summary ---\n",
      "ðŸ“± Total phones detected: 165\n",
      "ðŸ§â€â™‚ï¸ Total background people detected: 1855\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize Mediapipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load YOLOv8 (or replace with yolov10n.pt when ready)\n",
    "yolo = YOLO(\"yolov8n.pt\")  # Replace with \"yolov10n.pt\" if trained\n",
    "\n",
    "# Eye landmarks\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "score_history = deque(maxlen=10)\n",
    "distraction = 0\n",
    "\n",
    "# Counters\n",
    "phone_count = 0\n",
    "background_person_count = 0\n",
    "\n",
    "def eye_aspect_ratio(landmarks, eye_points, image_w, image_h):\n",
    "    p = []\n",
    "    for idx in eye_points:\n",
    "        lm = landmarks[idx]\n",
    "        x, y = int(lm.x * image_w), int(lm.y * image_h)\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "        p.append((x, y))\n",
    "    A = np.linalg.norm(np.array(p[1]) - np.array(p[5]))\n",
    "    B = np.linalg.norm(np.array(p[2]) - np.array(p[4]))\n",
    "    C = np.linalg.norm(np.array(p[0]) - np.array(p[3]))\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def is_blinking(ear, threshold=0.2):\n",
    "    return ear < threshold\n",
    "\n",
    "def compute_concentration_score(gaze, head_pose, blink):\n",
    "    score = 0.4 * gaze + 0.4 * head_pose + 0.2 * (0 if blink else 1)\n",
    "    return round(score * 100, 2)\n",
    "\n",
    "def bar(score, frame):\n",
    "    bar_x, bar_y = 30, 100\n",
    "    bar_width, bar_height = 200, 30\n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)\n",
    "    fill_width = int(score * bar_width / 100)\n",
    "    color = (0, 255, 0) if score > 40 else (0, 100, 255)\n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + fill_width, bar_y + bar_height), color, -1)\n",
    "    cv2.putText(frame, f\"{score}%\", (bar_x + bar_width + 10, bar_y + bar_height // 2 + 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "def draw_stop_button(frame):\n",
    "    button_color = (0, 0, 255)\n",
    "    button_text = \"STOP\"\n",
    "    button_pos = (frame.shape[1] - 120, 20)\n",
    "    button_size = (100, 50)\n",
    "    cv2.rectangle(frame, button_pos, (button_pos[0] + button_size[0], button_pos[1] + button_size[1]), button_color, -1)\n",
    "    cv2.putText(frame, button_text, (button_pos[0] + 10, button_pos[1] + 35),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    return button_pos, button_size\n",
    "\n",
    "def is_button_clicked(event, x, y, flags, param):\n",
    "    global stop_requested\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        bx, by, bw, bh = param\n",
    "        if bx <= x <= bx + bw and by <= y <= by + bh:\n",
    "            stop_requested = True\n",
    "\n",
    "# Start camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "stop_requested = False\n",
    "cv2.namedWindow(\"Concentration Tracker\")\n",
    "\n",
    "# Mouse callback\n",
    "cv2.setMouseCallback(\"Concentration Tracker\", is_button_clicked, param=(0, 0, 0, 0))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or stop_requested:\n",
    "        break\n",
    "\n",
    "    # Draw UI background\n",
    "    ui_bg = frame.copy()\n",
    "    cv2.rectangle(ui_bg, (0, 0), (frame.shape[1], 150), (30, 30, 30), -1)\n",
    "    cv2.addWeighted(ui_bg, 0.6, frame, 0.4, 0, frame)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_h, image_w, _ = frame.shape\n",
    "\n",
    "    # Mediapipe face mesh\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS)\n",
    "            landmarks = face_landmarks.landmark\n",
    "            left_ear = eye_aspect_ratio(landmarks, LEFT_EYE, image_w, image_h)\n",
    "            right_ear = eye_aspect_ratio(landmarks, RIGHT_EYE, image_w, image_h)\n",
    "            avg_ear = (left_ear + right_ear) / 2\n",
    "            blink = is_blinking(avg_ear)\n",
    "\n",
    "            gaze_score = 1  # Placeholder\n",
    "            head_score = 1  # Placeholder\n",
    "            concentration = compute_concentration_score(gaze_score, head_score, blink)\n",
    "            score_history.append(concentration)\n",
    "            smooth_score = int(np.mean(score_history))\n",
    "            bar(smooth_score, frame)\n",
    "            cv2.putText(frame, f\"Concentration: {smooth_score}%\", (30, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            if blink:\n",
    "                cv2.putText(frame, \"BLINKING\", (30, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 150, 255), 2)\n",
    "\n",
    "    # YOLO object detection\n",
    "    yolo_results = yolo.predict(source=frame, save=False, conf=0.4, verbose=False)\n",
    "    for r in yolo_results:\n",
    "        for box in r.boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            label = yolo.names[cls]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            conf = box.conf[0]\n",
    "            if label == \"cell phone\":\n",
    "                phone_count += 1\n",
    "                color = (0, 0, 255)\n",
    "            elif label == \"person\":\n",
    "                background_person_count += 1\n",
    "                color = (255, 255, 0)\n",
    "            else:\n",
    "                continue\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Draw STOP button\n",
    "    button_pos, button_size = draw_stop_button(frame)\n",
    "    cv2.setMouseCallback(\"Concentration Tracker\", is_button_clicked,\n",
    "                         param=(button_pos[0], button_pos[1], button_size[0], button_size[1]))\n",
    "\n",
    "    cv2.imshow(\"Concentration Tracker\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or stop_requested:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Show counts after stopping\n",
    "print(\"\\n--- Detection Summary ---\")\n",
    "print(f\"ðŸ“± Total phones detected: {phone_count}\")\n",
    "print(f\"ðŸ§â€â™‚ï¸ Total background people detected: {background_person_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928b2f3-14b0-4edf-85b8-5a83907b92bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (FaceRecog)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
